\documentclass{beamer}
\usepackage{beamerthemesplit}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}
\usepackage{xcolor,colortbl}
\usepackage[square]{natbib}
%%\newcommand{\newblock}{}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usetheme[pageofpages=of,% String used between the current page and the
                         % total page count.
          bullet=circle,% Use circles instead of squares for bullets.
          titleline=true,% Show a line below the frame title.
          alternativetitlepage=true,% Use the fancy title page.
          ]{Torino}
\definecolor{light-green}{RGB}{144,238,144}

\begin{document}
\author{{\bf Casey Stella}}
\institute[Hortonworks]{\includegraphics[width=40px,height=17px]{logo}}
\title{{\bf Natural Language Processing with Mahout}}
\date{June 26, 2013} 

\frame{\titlepage} 

\frame{\frametitle{Table of Contents}\tableofcontents} 


\section{Preliminaries}

\frame{\frametitle{Introduction}
\begin{itemize}
\item I'm a Systems Architect at Hortonworks
\item Prior to this, I've spent my time and had a lot of fun
  \begin{itemize}
  \item Doing data mining on medical data at Explorys using the Hadoop ecosystem
  \item Doing signal processing on seismic data at Ion Geophysical using MapReduce
  \item Being a graduate student in the Math department at Texas A\&M in algorithmic complexity theory
  \end{itemize}
\item I'm going to talk about Natural Language Processing in the Hadoop ecosystem.
\item I'm going to go over Apache Mahout in general and then focus on Topic Models.
\end{itemize}
}

\frame{\frametitle{Natural Language Processing}
Peter Norvig, the Director of Research at Google, said in the Amazon book review for the book ``Statistical Natural Language Processing''\footnote{http://www.amazon.com/review/R3GSYXSKRU8V17}
by Manning and Schuetze 
\begin{quote}
If someone told me I had to make a million bucks in one year, and I could only refer to one book to do it, I'd grab a copy of this book and start a web text-processing company.
\end{quote}
}

\section{Mahout $\to$ An Overview}

\frame{\frametitle{Apache Mahout}
\begin{itemize}
\item Apache Mahout is a
  \begin{itemize}
  \item Library of stand-alone scalable and distributed machine learning algorithms
  \item Library of high performance math and primitive collections useful in machine learning 
  \item Library of primitive distributed statistical and linear algebraic operations useful in machine learning
  \end{itemize}
\item The distributed algorithms are able to be run on Hadoop via a set of stand-alone helper utilities as well as providing an API.
\end{itemize}
}

%%\frame{\frametitle{Classes of Algorithms Included}
%%\begin{itemize}
%%\item Mahout includes distributed algorithms for
%%  \begin{itemize}
%%  \item Classification
%%  \item Clustering
%%  \item Pattern Matching/Frequent Itemset Mining
%%  \item Recommendation Engines/Collaborative Filtering
%%  \end{itemize}
%%\end{itemize}
%%}

\frame{\frametitle{Selection of Available Algorithms}
  \begin{tabular}{| l | l | }
  \hline
  {\bf Type} & {\bf Algorithm} \\ \hline
  Linear Algebra & Stochastic Gradient Descent \\ \hline
  Linear Algebra & Stochastic Singular Value Decomposition\\ \hline
  \cellcolor{pink}{Classification} & \cellcolor{pink}{Random Forests} \\ \hline
  \cellcolor{pink}{Classification} & \cellcolor{pink}{Na\"{i}ve Bayesian} \\ \hline
  \cellcolor{pink}{Classification} & \cellcolor{pink}{Hidden Markov Models} \\ \hline
  \cellcolor{light-green}{Clustering} & \cellcolor{light-green}{Normal and Fuzzy K-Means} \\ \hline
  \cellcolor{light-green}{Clustering} & \cellcolor{light-green}{Expectation Maximization} \\ \hline
  \cellcolor{light-green}{Clustering} & \cellcolor{light-green}{Dirichlet Process Clustering} \\ \hline
  \cellcolor{light-green}{Clustering} & \cellcolor{light-green}{Latent Dirichlet Allocation} \\ \hline
  \cellcolor{light-green}{Clustering} & \cellcolor{light-green}{Spectral Clustering} \\ \hline
  \cellcolor{light-green}{Clustering} & \cellcolor{light-green}{MinHash Clustering} \\ \hline 
  \cellcolor{gray}{Pattern Mining} & \cellcolor{gray}{Parallel FP Growth} \\ \hline
  \end{tabular}
}

\section{NLP with Mahout $\to$ An Example}

\subsection{Ingesting Data}
\frame{\frametitle{Ingesting a Corpus of Documents}
\begin{itemize}
\item Mahout provides a number of utilities to allow one to ingest data into Hadoop in the format expected by the ML algorithms
\item The basic pattern is 
  \begin{itemize}
  \item Convert the documents to SequenceFiles via the {\bf seqdirectory} command
  \item Convert those sequence files to a set of sparse vectors using {\bf seq2sparse} by computing a dictionary, assigning integers for words, computing feature weights and creating a vector for each document using the word-integer mapping and feature-weight.
  \item Convert the keys associated with the sparse vectors to incrementing integers using the {\bf rowid} command
  \end{itemize}
\end{itemize}
}

\frame{\frametitle{Converting a Sequence File to a set of Vectors}
\begin{itemize}
\item Create a sparse set of vectors using the mahout utility {\bf seq2sparse}.
\item The {\bf seq2sparse} command allows you to specify:
  \begin{center}
    \begin{tabular}{| l | p{7cm} |}
    \hline
    \texttt{-wt} & The weighting method used: tf or tfidf \\ \hline
    \texttt{--minSupport} & The minimum number of times a term has to occur to exist in the document \\ \hline
    \texttt{--norm} & An integer $k > 0$ indicating the $L_k$ metric to be used to normalize the vectors.\\ \hline
    \texttt{-a} & The analyzer to use.\\ \hline
    \end{tabular}
  \end{center}

\end{itemize}
}


\subsection{Topic Models}


\frame{\frametitle{Topic Models}
\begin{itemize}
\item Topic modeling is intended to find a set of broad themes or ``topics'' from a corpus of documents.
\item Documents contain multiple topics and, indeed, can be considered a ``mixture'' of topics.
\item Probabalistic topic modeling algorithms attempt to determine the set of topics and mixture of topics per-document in an unsupervised way.
\item Consider a collection of newspaper articles, topics may be ``sports'', ``politics'', etc.
\end{itemize}
}

\frame{\frametitle{High Level: Latent Dirichlet Allocation}
\begin{itemize}
\item Topics are determined by looking at how often words appear together in the same document
\item Each document is associated a probability distribution over the set of topics
\item Latent Dirichlet Allocation (LDA) is a statistical topic model which learns
  \begin{itemize}
  \item what the topics are
  \item which documents employ said topics and at what distribution
  \end{itemize}
\end{itemize}
}

\frame{\frametitle{Latent Dirichlet Allocation: A Parable}
Tim is the owner of an independent record shop and is interested in finding the 
natural genres of music by considering natural groupings based on what people buy.  
So, Tim logs the records people buy  and who buys them.  Tim does not know the genres 
and he doesn't know the different genres each customer likes.

Tim chooses that he wants to learn $K$ genres and let a set of records define a given
genre.  He can then assign a label to the genre by eyeballing the records in the genres.

\begin{itemize}
\item Words correspond to records
\item Documents correspond to people
\item Topics correspond to genres and are represented by $\{records\}$.
\end{itemize}
}

\frame{\frametitle{Latent Dirichlet Allocation: A Parable}
Tim starts by making a guess as to why records are bought by certain people.  For
example, he assumes that customers who buy record A have interest in the same genre
and therefore record A must be a representative of that genre.  Of course,
this assumption is very likely to be incorrect, so he needs to improve in the face of 
better data.
}

\frame{\frametitle{Latent Dirichlet Allocation: A Parable}
He comes up with the following scheme:
\begin{itemize}
\item Pick a record and a customer who bought that record.
\item Guess why the record was bought by the customer.
\item Other records that the customer bought are likely of the same genre. In other words, the more records that are bought by the same customer, the more likely that those records are part of the same genre.
\item Make a new guess as to why the customer bought that record, choosing a genre with some probability according to how likely Tim thinks it is.
\end{itemize}
}

\frame{\frametitle{Latent Dirichlet Allocation: A Parable}
Tim goes through each customer purchase over and over again.  His guesses keep getting 
better because he starts to notice patterns (i.e. people who buy the same records are 
likely interested in the same genres).  Eventually he feels like he's refined his model
enough and is ready to draw conclusions:
\begin{itemize}
\item For each genre, you can count the records assigned to that genre to figure out what records are associated with the genre.
\item By looking at the records in the genre, you can give the genre a label.
\item For each customer $D$ and genre $T$, you can compute the proportions of records who were bought by $D$ because they liked genre $T$.  These give you a representation of customer $D$.  For example, you might learn that records bought by Jim consist of 10\% ``Easy Listening'', ``20\% Rap'', and 70\% ``Country \& Western''.
\end{itemize}

}


%%\frame{\frametitle{Latent Dirichlet Allocation $\to$ Example}
%%\begin{itemize}
%%\item Consider sentences:
%%  \begin{itemize}
%%  \item I like basketball and football.
%%  \item Tim drank gatorade after football practice.
%%  \item John drank gatorade and thinks it tastes terrible.
%%  \end{itemize}
%%\item For the topics:
%%  \begin{itemize}
%%  \item {\em Topic 1} $\to$ basketball, football
%%  \item {\em Topic 2} $\to$ gatorade, drank
%%  \end{itemize}
%%\item And sentences:
%%  \begin{itemize}
%%  \item Sentence 1 is 100\% Topic 1
%%  \item Sentence 3 is 100\% Topic 2
%%  \item Sentence 2 is 50\% Topic 1 and 50\% Topic 2
%%  \end{itemize}
%%\end{itemize}
%%}

\frame{\frametitle{LDA in Mahout}
\begin{itemize}
\item Original implementation followed the original implementation proposed by \cite{Blei:2003:LDA}.
  \begin{itemize}
  \item The problem, in part, the amount of information sent out of the mappers scaled with the product of the number of terms in the vocabulary and number of topics.
  \item On a 1 billion non-zero entry corpus, for 200 topics, original implementation sent 2.5 TB of data from the mappers {\em per iteration}.
  \item Recently (as of 0.6 [MAHOUT-897]) moved to Collapsed Variational Bayes by \cite{Asuncion:2009:SIT}.
  \item About 15x faster than original implementation
  \end{itemize}
\end{itemize}
}

\frame{\frametitle{LDA in Mahout}
\begin{itemize}
\item Input data is expected to be a sparse vector
\item Ingestion Pipeline from a document directory
  \begin{itemize}
  \item {\bf seqdirectory} $\to$ Transforms docs to a sequence file containing a document per entry
  \item {\bf seq2sparse} $\to$ Transforms to a sequence file of sparse vectors as well as a dictionary of terms
  \item {\bf rowid} $\to$  Transforms the keys of the sparse vectors into integers 
  \end{itemize}
\item LDA expects the matrix as input as well as the dictionary
\end{itemize}
}

\frame{\frametitle{LDA in Mahout}
\begin{itemize}
\item The {\bf cvb} tool will run the LDA algorithm
\item Input: sequence file of SparseVectors of word counts weighted by term frequency
\item Output: Topic model
\item Parameters:
   \begin{center}
   \scalebox{0.9}{
    \begin{tabular}{| l | p{10cm} |}
    \hline
    {\texttt -k} & The number of topics \\ \hline
    {\texttt -nt} & The number of unique features defined by the input document vectors \\ \hline
    {\texttt -maxIter} & The maximum number of iterations.\\ \hline
    {\texttt -mipd} & The maximum number of iterations per document\\ \hline
    {\texttt -a} & Smoothing for the document topic distribution; should be about $\frac{50}{k}$, with k being the number of topics.\\ \hline
    {\texttt -e} & Smoothing for the term topic distribution\\ \hline
  \end{tabular}
  }
  \end{center}
\end{itemize}
}

\frame{\frametitle{LDA in Mahout $\to$ Topics}
\begin{center}
   \scalebox{0.85}{
  \begin{tabular}{| l | l | l | l | l | }
  \hline
  {\bf Topic 1} & {\bf Topic 2} & {\bf Topic 3} & {\bf Topic 4} & {\bf Topic 5} \\ \hline
\color{red}{bush}&\color{red}{sharon}&\color{red}{sharon}&\color{red}{sharon}&\color{red}{sharon} \\ \hline
\color{red}{election}&administration&\color{red}{disengagement}&bush&\color{red}{disengagement}\\ \hline
palestinians&\color{red}{bush}&administration&administration&us\\ \hline
hand&us&election&\color{red}{settlement}&bush\\ \hline
year&\color{red}{endorse}&state&\color{red}{plan}&\color{red}{ west}\\ \hline
\color{red}{fence}&leader&bush&palestinians&election\\ \hline
roadmap&iraq&hand&disengagement&solution\\ \hline
arafat&year&\color{red}{ conflict}&solution&hand\\ \hline
have&election&year&election&day\\ \hline
conflict&transfer&us&fence&year\\ \hline
  \end{tabular}
  }
\end{center}
}
\section{Questions \& Bibliography}
\frame{\frametitle{Questions \& Bibliography}
Thanks for your attention!  Questions? \\
\begin{itemize}
\item Code and scripts for this talk at http://github.com/cestella/NLPWithMahout
\item Find me at http://caseystella.com 
\item Twitter handle: @casey\_stella 
\item Email address: cstella@hortonworks.com
\end{itemize}
{\bf BIBLIOGRAPHY}
\bibliographystyle{abbrvnat}
\bibliography{NLP_with_Mahout}

}

\end{document}
